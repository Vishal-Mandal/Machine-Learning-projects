{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cb06abb",
   "metadata": {},
   "source": [
    "# Using Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25c2cf2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69a3c50e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 19503 entries, 0 to 19502\n",
      "Data columns (total 16 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   City        19503 non-null  object \n",
      " 1   Datetime    19503 non-null  object \n",
      " 2   PM2.5       18061 non-null  float64\n",
      " 3   PM10        18071 non-null  float64\n",
      " 4   NO          18738 non-null  float64\n",
      " 5   NO2         18795 non-null  float64\n",
      " 6   NOx         19386 non-null  float64\n",
      " 7   NH3         19427 non-null  float64\n",
      " 8   CO          19480 non-null  float64\n",
      " 9   SO2         17907 non-null  float64\n",
      " 10  O3          18161 non-null  float64\n",
      " 11  Benzene     19444 non-null  float64\n",
      " 12  Toluene     19438 non-null  float64\n",
      " 13  Xylene      9352 non-null   float64\n",
      " 14  AQI         18066 non-null  float64\n",
      " 15  AQI_Bucket  18066 non-null  object \n",
      "dtypes: float64(13), object(3)\n",
      "memory usage: 2.4+ MB\n"
     ]
    }
   ],
   "source": [
    "Data = pd.read_csv(\"kolkata_hrdata.csv\")\n",
    "Data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80f406d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = []\n",
    "#target = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eacf37ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 19503 entries, 0 to 19502\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   Datetime  19503 non-null  object \n",
      " 1   PM2.5     18061 non-null  float64\n",
      " 2   PM10      18071 non-null  float64\n",
      " 3   CO        19480 non-null  float64\n",
      " 4   AQI       18066 non-null  float64\n",
      "dtypes: float64(4), object(1)\n",
      "memory usage: 762.0+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Datetime</th>\n",
       "      <th>PM2.5</th>\n",
       "      <th>PM10</th>\n",
       "      <th>CO</th>\n",
       "      <th>AQI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-04-10 10:00:00</td>\n",
       "      <td>34.82</td>\n",
       "      <td>54.45</td>\n",
       "      <td>1.13</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-04-10 11:00:00</td>\n",
       "      <td>31.82</td>\n",
       "      <td>57.80</td>\n",
       "      <td>1.02</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-04-10 12:00:00</td>\n",
       "      <td>29.61</td>\n",
       "      <td>51.04</td>\n",
       "      <td>0.94</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-04-10 13:00:00</td>\n",
       "      <td>33.37</td>\n",
       "      <td>51.61</td>\n",
       "      <td>0.91</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-04-10 14:00:00</td>\n",
       "      <td>36.71</td>\n",
       "      <td>58.00</td>\n",
       "      <td>0.91</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2018-04-10 15:00:00</td>\n",
       "      <td>35.88</td>\n",
       "      <td>56.18</td>\n",
       "      <td>0.86</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2018-04-10 16:00:00</td>\n",
       "      <td>37.16</td>\n",
       "      <td>53.08</td>\n",
       "      <td>0.97</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2018-04-10 17:00:00</td>\n",
       "      <td>37.37</td>\n",
       "      <td>53.86</td>\n",
       "      <td>0.89</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2018-04-10 18:00:00</td>\n",
       "      <td>35.42</td>\n",
       "      <td>53.62</td>\n",
       "      <td>0.79</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2018-04-10 19:00:00</td>\n",
       "      <td>36.37</td>\n",
       "      <td>57.87</td>\n",
       "      <td>0.99</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2018-04-10 20:00:00</td>\n",
       "      <td>37.21</td>\n",
       "      <td>65.49</td>\n",
       "      <td>1.13</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2018-04-10 21:00:00</td>\n",
       "      <td>33.65</td>\n",
       "      <td>60.85</td>\n",
       "      <td>1.07</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2018-04-10 22:00:00</td>\n",
       "      <td>29.83</td>\n",
       "      <td>58.86</td>\n",
       "      <td>1.19</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2018-04-10 23:00:00</td>\n",
       "      <td>32.09</td>\n",
       "      <td>65.36</td>\n",
       "      <td>1.18</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2018-04-11 00:00:00</td>\n",
       "      <td>36.15</td>\n",
       "      <td>63.77</td>\n",
       "      <td>1.06</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2018-04-11 01:00:00</td>\n",
       "      <td>26.67</td>\n",
       "      <td>63.68</td>\n",
       "      <td>1.11</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2018-04-11 02:00:00</td>\n",
       "      <td>25.24</td>\n",
       "      <td>63.58</td>\n",
       "      <td>1.08</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2018-04-11 03:00:00</td>\n",
       "      <td>23.06</td>\n",
       "      <td>61.27</td>\n",
       "      <td>1.09</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2018-04-11 04:00:00</td>\n",
       "      <td>22.75</td>\n",
       "      <td>59.56</td>\n",
       "      <td>1.09</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2018-04-11 05:00:00</td>\n",
       "      <td>27.50</td>\n",
       "      <td>54.61</td>\n",
       "      <td>0.83</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Datetime  PM2.5   PM10    CO   AQI\n",
       "0   2018-04-10 10:00:00  34.82  54.45  1.13   NaN\n",
       "1   2018-04-10 11:00:00  31.82  57.80  1.02   NaN\n",
       "2   2018-04-10 12:00:00  29.61  51.04  0.94   NaN\n",
       "3   2018-04-10 13:00:00  33.37  51.61  0.91   NaN\n",
       "4   2018-04-10 14:00:00  36.71  58.00  0.91   NaN\n",
       "5   2018-04-10 15:00:00  35.88  56.18  0.86   NaN\n",
       "6   2018-04-10 16:00:00  37.16  53.08  0.97   NaN\n",
       "7   2018-04-10 17:00:00  37.37  53.86  0.89   NaN\n",
       "8   2018-04-10 18:00:00  35.42  53.62  0.79   NaN\n",
       "9   2018-04-10 19:00:00  36.37  57.87  0.99   NaN\n",
       "10  2018-04-10 20:00:00  37.21  65.49  1.13   NaN\n",
       "11  2018-04-10 21:00:00  33.65  60.85  1.07   NaN\n",
       "12  2018-04-10 22:00:00  29.83  58.86  1.19   NaN\n",
       "13  2018-04-10 23:00:00  32.09  65.36  1.18   NaN\n",
       "14  2018-04-11 00:00:00  36.15  63.77  1.06   NaN\n",
       "15  2018-04-11 01:00:00  26.67  63.68  1.11  60.0\n",
       "16  2018-04-11 02:00:00  25.24  63.58  1.08  60.0\n",
       "17  2018-04-11 03:00:00  23.06  61.27  1.09  60.0\n",
       "18  2018-04-11 04:00:00  22.75  59.56  1.09  60.0\n",
       "19  2018-04-11 05:00:00  27.50  54.61  0.83  60.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature = Data\n",
    "feature = feature.drop('City',axis=1)\n",
    "feature = feature.drop('NO',axis=1)\n",
    "feature = feature.drop('NO2',axis=1)\n",
    "feature = feature.drop('NOx',axis=1)\n",
    "feature = feature.drop('NH3',axis=1)\n",
    "feature = feature.drop('SO2',axis=1)\n",
    "feature = feature.drop('O3',axis=1)\n",
    "feature = feature.drop('Toluene',axis=1)\n",
    "feature = feature.drop('Benzene',axis=1)\n",
    "feature = feature.drop('Xylene',axis=1)\n",
    "feature = feature.drop('AQI_Bucket',axis=1)\n",
    "feature.info()\n",
    "feature.head(20)#Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "243a958b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(feature)\n",
    "df.to_csv('myfile.csv')\n",
    "#X = feature[:,:-1]\n",
    "#Y = feature[:,-1]#y=feature[]\n",
    "#print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6261b3",
   "metadata": {},
   "source": [
    "## Node Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76b97ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node():\n",
    "    def __init__(self, feature_index=None, threshold=None, left=None, right=None, var_red=None, value=None):\n",
    "        ''' constructor ''' \n",
    "        \n",
    "        # for decision node\n",
    "        self.feature_index = feature_index\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.var_red = var_red\n",
    "        \n",
    "        # for leaf node\n",
    "        self.value = value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02f2351",
   "metadata": {},
   "source": [
    "## Tree Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7988949c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeRegressor():\n",
    "    def __init__(self, min_samples_split=2, max_depth=2):\n",
    "        ''' constructor '''\n",
    "        \n",
    "        # initialize the root of the tree \n",
    "        self.root = None\n",
    "        \n",
    "        # stopping conditions\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_depth = max_depth\n",
    "        \n",
    "    def build_tree(self, dataset, curr_depth=0):\n",
    "        ''' recursive function to build the tree '''\n",
    "        \n",
    "        X, Y = dataset[:,:-1], dataset[:,-1]\n",
    "        num_samples, num_features = np.shape(X)\n",
    "        best_split = {}\n",
    "        # split until stopping conditions are met\n",
    "        if num_samples>=self.min_samples_split and curr_depth<=self.max_depth:\n",
    "            # find the best split\n",
    "            best_split = self.get_best_split(dataset, num_samples, num_features)\n",
    "            # check if information gain is positive\n",
    "            if best_split[\"var_red\"]>0:\n",
    "                # recur left\n",
    "                left_subtree = self.build_tree(best_split[\"dataset_left\"], curr_depth+1)\n",
    "                # recur right\n",
    "                right_subtree = self.build_tree(best_split[\"dataset_right\"], curr_depth+1)\n",
    "                # return decision node\n",
    "                return Node(best_split[\"feature_index\"], best_split[\"threshold\"], \n",
    "                            left_subtree, right_subtree, best_split[\"var_red\"])\n",
    "        \n",
    "        # compute leaf node\n",
    "        leaf_value = self.calculate_leaf_value(Y)\n",
    "        # return leaf node\n",
    "        return Node(value=leaf_value)\n",
    "    \n",
    "    def get_best_split(self, dataset, num_samples, num_features):\n",
    "        ''' function to find the best split '''\n",
    "        \n",
    "        # dictionary to store the best split\n",
    "        best_split = {}\n",
    "        max_var_red = -float(\"inf\")\n",
    "        # loop over all the features\n",
    "        for feature_index in range(num_features):\n",
    "            feature_values = dataset[:, feature_index]\n",
    "            possible_thresholds = np.unique(feature_values)\n",
    "            # loop over all the feature values present in the data\n",
    "            for threshold in possible_thresholds:\n",
    "                # get current split\n",
    "                dataset_left, dataset_right = self.split(dataset, feature_index, threshold)\n",
    "                # check if childs are not null\n",
    "                if len(dataset_left)>0 and len(dataset_right)>0:\n",
    "                    y, left_y, right_y = dataset[:, -1], dataset_left[:, -1], dataset_right[:, -1]\n",
    "                    # compute information gain\n",
    "                    curr_var_red = self.variance_reduction(y, left_y, right_y)\n",
    "                    # update the best split if needed\n",
    "                    if curr_var_red>max_var_red:\n",
    "                        best_split[\"feature_index\"] = feature_index\n",
    "                        best_split[\"threshold\"] = threshold\n",
    "                        best_split[\"dataset_left\"] = dataset_left\n",
    "                        best_split[\"dataset_right\"] = dataset_right\n",
    "                        best_split[\"var_red\"] = curr_var_red\n",
    "                        max_var_red = curr_var_red\n",
    "                        \n",
    "        # return best split\n",
    "        return best_split\n",
    "    \n",
    "    def split(self, dataset, feature_index, threshold):\n",
    "        ''' function to split the data '''\n",
    "        \n",
    "        dataset_left = np.array([row for row in dataset if row[feature_index]<=threshold])\n",
    "        dataset_right = np.array([row for row in dataset if row[feature_index]>threshold])\n",
    "        return dataset_left, dataset_right\n",
    "    \n",
    "    def variance_reduction(self, parent, l_child, r_child):\n",
    "        ''' function to compute variance reduction '''\n",
    "        \n",
    "        weight_l = len(l_child) / len(parent)\n",
    "        weight_r = len(r_child) / len(parent)\n",
    "        reduction = np.var(parent) - (weight_l * np.var(l_child) + weight_r * np.var(r_child))\n",
    "        return reduction\n",
    "    \n",
    "    def calculate_leaf_value(self, Y):\n",
    "        ''' function to compute leaf node '''\n",
    "        \n",
    "        val = np.mean(Y)\n",
    "        return val\n",
    "                \n",
    "    def print_tree(self, tree=None, indent=\" \"):\n",
    "        ''' function to print the tree '''\n",
    "        \n",
    "        if not tree:\n",
    "            tree = self.root\n",
    "\n",
    "        if tree.value is not None:\n",
    "            print(tree.value)\n",
    "\n",
    "        else:\n",
    "            print(\"X_\"+str(tree.feature_index), \"<=\", tree.threshold, \"?\", tree.var_red)\n",
    "            print(\"%sleft:\" % (indent), end=\"\")\n",
    "            self.print_tree(tree.left, indent + indent)\n",
    "            print(\"%sright:\" % (indent), end=\"\")\n",
    "            self.print_tree(tree.right, indent + indent)\n",
    "    \n",
    "    def fit(self, X, Y):\n",
    "        ''' function to train the tree '''\n",
    "        \n",
    "        dataset = np.concatenate((X, Y), axis=1)\n",
    "        self.root = self.build_tree(dataset)\n",
    "        \n",
    "    def make_prediction(self, x, tree):\n",
    "        ''' function to predict new dataset '''\n",
    "        \n",
    "        if tree.value!=None: return tree.value\n",
    "        feature_val = x[tree.feature_index]\n",
    "        if feature_val<=tree.threshold:\n",
    "            return self.make_prediction(x, tree.left)\n",
    "        else:\n",
    "            return self.make_prediction(x, tree.right)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        ''' function to predict a single data point '''\n",
    "        \n",
    "        preditions = [self.make_prediction(x, self.root) for x in X]\n",
    "        return preditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0471c302",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = feature.iloc[:, :-1].values\n",
    "Y = feature.iloc[:, -1].values.reshape(-1,1)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=.2, random_state=41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ffa28ab8",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'var_red'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\VISHAL~1\\AppData\\Local\\Temp/ipykernel_10644/4064625508.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mregressor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDecisionTreeRegressor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmin_samples_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mregressor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mregressor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint_tree\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\VISHAL~1\\AppData\\Local\\Temp/ipykernel_10644/167053609.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, Y)\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m         \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 109\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_tree\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    110\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmake_prediction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtree\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\VISHAL~1\\AppData\\Local\\Temp/ipykernel_10644/167053609.py\u001b[0m in \u001b[0;36mbuild_tree\u001b[1;34m(self, dataset, curr_depth)\u001b[0m\n\u001b[0;32m     21\u001b[0m             \u001b[0mbest_split\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_best_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m             \u001b[1;31m# check if information gain is positive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mbest_split\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"var_red\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m                 \u001b[1;31m# recur left\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m                 \u001b[0mleft_subtree\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_tree\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbest_split\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"dataset_left\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurr_depth\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'var_red'"
     ]
    }
   ],
   "source": [
    "regressor = DecisionTreeRegressor(min_samples_split=3, max_depth=3)\n",
    "regressor.fit(X_train,Y_train)\n",
    "regressor.print_tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9add34b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    ".\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
